# -*- coding: utf-8 -*-
"""Copy of SimpleVAE.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ir8aosEr5fDmgQcULf3Jjk4Q10rVEIk7
"""

import torch
import torchvision
import matplotlib.pyplot as plt
import torch.nn as nn
import numpy as np
import torch.nn.functional as F

"""# AutoEncoder"""

class Encoder(nn.Module):
    def __init__ (self, input_dim, latent_dim1, latent_dim2):
        super(Encoder, self).__init__()
        self.linear1 = nn.Linear(input_dim, latent_dim1)
        self.linear2 = nn.Linear(latent_dim1, latent_dim2)
        
    def forward(self, x):
        a = self.linear1(x)
        a = torch.relu(a)
        a = self.linear2(a)
        a = torch.relu(a)
        return a
    def forward(self, x):
        x = torch.flatten(x, start_dim=1)
        x = F.relu(self.linear1(x))
        return self.linear2(x)

class Decoder(nn.Module):
    def __init__ (self, output_dim, latent_dim1, latent_dim2):
        super(Decoder, self).__init__()
        self.linear1 = nn.Linear(latent_dim2, latent_dim1)
        self.linear2 = nn.Linear(latent_dim1, output_dim)
        
    def forward(self, x):
        a = self.linear1(x)
        a = torch.relu(a)
        a = self.linear2(a)
        a = torch.relu(a)
        return a

class Autoencoder(nn.Module):
    def __init__(self, input_dim, latent_dim1, latent_dim2, output_dim):
        super(Autoencoder, self).__init__()
        self.encoder = Encoder(input_dim, latent_dim1, latent_dim2)
        self.decoder = Decoder(output_dim, latent_dim1, latent_dim2)

    def forward(self, x):
        z = self.encoder(x)
        output = self.decoder(z)
        return output

"""# Variational AutoEncoder"""

class VAE(nn.Module):
    def __init__(self, input_dim, latent_dim1, latent_dim2, output_dim):
        super(VAE, self).__init__()
        self.encoder1 = nn.Linear(input_dim, latent_dim1)
        self.encoder2 = nn.Linear(latent_dim1, latent_dim2*2)
        self.decoder1 = nn.Linear(latent_dim2, latent_dim1)
        self.decoder2 = nn.Linear(latent_dim1, output_dim)

    def reparameterize(self, mu, log_var):
        std = torch.exp(0.5*log_var) 
        eps = torch.randn_like(std) 
        sample = mu + (eps * std) 
        return sample
 
    def forward(self, x):
        x = F.relu(self.encoder1(x))
        x = self.encoder2(x).view(-1, 2, latent_dim2)
        mu = x[:, 0, :]
        log_var = x[:, 1, :]
        z = self.reparameterize(mu, log_var)
        x = F.relu(self.decoder1(z))
        reconstruction = torch.sigmoid(self.decoder2(x))
        return reconstruction, mu, log_var

"""# How to train"""

def dataloader(raw_data, batch_size):
    data = torch.utils.data.DataLoader(
        raw_data,
        batch_size=batch_size,
        shuffle=True)
    return data

def train(autoencoder, data, epochs):
    opt = torch.optim.Adam(autoencoder.parameters())
    for epoch in range(epochs):
        for x, y in data:
            opt.zero_grad()
            x_hat = autoencoder(x)
            loss = ((x - x_hat)**2).sum()
            loss.backward()
            opt.step()
    return autoencoder

"""
    How to run AE:
    autoencoder = Autoencoder(input_dim, latent_dim1, latent_dim2, output_dim)
    AE = train(autoencoder, data)
    How to run VAE:
    vae = VAE(latent_dims)
    VAutoencoder = train(vae, data)
"""